{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os , glob\n",
    "import os.path\n",
    "\n",
    "from shutil import copyfile\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"..\\\\data\\\\\"\n",
    "\n",
    "df_path = root_path + \"full_data_description.csv\"\n",
    "\n",
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter only good claims (where the transcript is matched the cards that the player claimed to place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_claims = df[\"MATCH(ASR,CLAIM TEXT)\"] == True\n",
    "df = df[good_claims]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train-test\n",
    " - By subjetcs\n",
    " - 70% - train , 30% - test \n",
    " - Split each task to support and query sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = round(len(df)*0.7)\n",
    "TEST_SIZE = len(df)-TRAIN_SIZE\n",
    "\n",
    "print(\"train size: {} \\ntest size: {}\".format(TRAIN_SIZE , TEST_SIZE))\n",
    "print(\"Amount of claims:\" ,len(df))\n",
    "amount_of_claims_per_w = df.groupby('WorkerId').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2210)\n",
    "counter_train = 0\n",
    "counter_test = 0\n",
    "train_w = []\n",
    "test_w = []\n",
    "for w in amount_of_claims_per_w.sample(frac=1).iteritems():\n",
    "    if counter_train + w[1] <= TRAIN_SIZE:\n",
    "        counter_train += w[1]\n",
    "        train_w.append(w[0])\n",
    "    else:\n",
    "        counter_test += w[1]\n",
    "        test_w.append(w[0])\n",
    "print(\"train size: {} \\ntest size: {}\".format(counter_train , counter_test))\n",
    "print(\"train workers: {} \\ntest workers: {}\".format(len(train_w) , len(test_w)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove subjects with less than 2 claims per class \n",
    "\n",
    "- Check amount of claims per subject\n",
    "- Save updated results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_idx  = df.index[df['WorkerId'] == '------'].tolist()\n",
    "temp_df =df\n",
    "for i in change_idx:\n",
    "    temp_df.loc[i,['WorkerId']] = df.loc[i,'name']\n",
    "\n",
    "\n",
    "temp_df['IsFalseClaim'] = 1-temp_df['IsTrueClaim'] \n",
    "filtered_workers=pd.pivot_table(temp_df, index=['WorkerId'],values=['IsTrueClaim','IsFalseClaim'],aggfunc=np.sum)\n",
    "filtered_workers = filtered_workers[filtered_workers['IsFalseClaim']>1][filtered_workers['IsTrueClaim']>1]\n",
    "# print(filtered_workers.sort_values(by=(['IsTrueClaim','IsFalseClaim'])[:]).to_string())\n",
    "# pd.set_option('display.max_rows', None)\n",
    "display(filtered_workers.sort_values(by=(['WorkerId'])[:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '------' in train_w:\n",
    "    def_set = 'train'\n",
    "else:\n",
    "    def_set = 'test'  \n",
    "for worker in filtered_workers.index:\n",
    "    if worker in train_w:\n",
    "        filtered_workers.at[worker,'set']='train'\n",
    "    elif worker in test_w:\n",
    "        filtered_workers.at[worker,'set']='test'\n",
    "    else:\n",
    "        filtered_workers\n",
    "        filtered_workers.at[worker,'set']=def_set\n",
    "    \n",
    "    # A bug in original division\n",
    "    try:\n",
    "        filtered_workers.drop(train_w[0],inplace=True)\n",
    "        filtered_workers.drop(test_w[0],inplace=True)\n",
    "    except:\n",
    "        \"\"       \n",
    "tr_list =filtered_workers.index[filtered_workers['set'] == 'train'].tolist()\n",
    "ts_list = filtered_workers.index[filtered_workers['set'] == 'test'].tolist()\n",
    "workers_list = tr_list+ts_list\n",
    "print(\"Amount of train workers = {}\\nAmount of test workers = {}\".format(len(tr_list),len(ts_list)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all filtered subject details to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_claims_df = df\n",
    "all_claims_df = all_claims_df.loc[all_claims_df['WorkerId'].isin(workers_list)]\n",
    "all_claims_df['path'] = all_claims_df['name']+'/'+all_claims_df['file_name']\n",
    "all_claims_df.to_csv(root_path+\"all_sampels_by_tasks\\\\samples_description.csv\")\n",
    "all_claims_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy files to folder by train-test - > subject -> true/false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "division_path = root_path + \"train_test_division\\\\\"\n",
    "os.mkdir(division_path)\n",
    "train_dir = division_path + \"train\\\\\"\n",
    "os.mkdir(train_dir)\n",
    "test_dir = division_path + \"test\\\\\"\n",
    "os.mkdir(test_dir)\n",
    "counterF_tr = 0\n",
    "counterT_tr = 0\n",
    "counterN_tr = 0\n",
    "counterF_ts = 0\n",
    "counterT_ts = 0\n",
    "\n",
    "is_train = 0\n",
    "\n",
    "paths = df.path\n",
    "names = df.file_name\n",
    "subjects =df.WorkerId\n",
    "claims = df.IsTrueClaim\n",
    "\n",
    "for claim_path, name, w, claim in zip(paths, names, subjects,claims):\n",
    "        if w in tr_list: # A train subject\n",
    "            if not path.isdir(train_dir+w):\n",
    "                os.mkdir(train_dir+w)\n",
    "            is_train = 1\n",
    "            if not path.isdir(train_dir+w+\"\\\\true_claims\\\\\"):\n",
    "                os.mkdir(train_dir+w+\"\\\\true_claims\\\\\")\n",
    "                os.mkdir(train_dir+w+\"\\\\false_claims\\\\\")\n",
    "\n",
    "        elif w in ts_list: # A test subject\n",
    "            if not path.isdir(test_dir+w):\n",
    "                os.mkdir(test_dir+w)\n",
    "            is_train = 0\n",
    "            if not path.isdir(test_dir+w+\"\\\\true_claims\\\\\"):\n",
    "                os.mkdir(test_dir+w+\"\\\\true_claims\\\\\")\n",
    "                os.mkdir(test_dir+w+\"\\\\false_claims\\\\\")\n",
    "        \n",
    "        else: # '------' worker Id\n",
    "            is_train = -1\n",
    "        if is_train == 1:\n",
    "            if not path.isdir(train_dir+w):\n",
    "                os.mkdir(train_dir+w)\n",
    "            if claim:\n",
    "                counterT_tr += 1\n",
    "                copyfile(claim_path,train_dir+w+\"\\\\true_claims\\\\\" + name)\n",
    "            elif not claim:\n",
    "                counterF_tr += 1\n",
    "                copyfile(claim_path, train_dir+w+\"\\\\false_claims\\\\\" + name)\n",
    "        elif is_train == 0:\n",
    "            if not path.isdir(test_dir+w):\n",
    "                os.mkdir(test_dir+w)\n",
    "            if claim:\n",
    "                counterT_ts += 1\n",
    "                copyfile(claim_path, test_dir+w+\"\\\\true_claims\\\\\" + name)\n",
    "            elif not claim:\n",
    "                counterF_ts += 1\n",
    "                copyfile(claim_path, test_dir+w+\"\\\\false_claims\\\\\" + name)\n",
    "\n",
    "print(\"TRAIN:\\ntrue: {} ,false: {}, {}\".format(counterT_tr,counterF_tr,counterF_tr/(counterT_tr+counterF_tr)))\n",
    "print(\"TEST:\\ntrue: {} ,false: {}, {}\".format(counterT_ts,counterF_ts,counterF_ts/(counterT_ts+counterF_ts)))\n",
    "\n",
    "print(\"\\nTRUE: {}\\nFALSE: {}\".format(counterT_tr +counterT_ts ,counterF_tr,counterF_ts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "division_path = root_path + \"train_test_division\\\\\"\n",
    "train_dir = division_path + \"train\\\\\"\n",
    "test_dir = division_path + \"test\\\\\"\n",
    "\n",
    "\n",
    "def create_df(directory,type=\"\"):\n",
    "    files = []\n",
    "    paths = []\n",
    "    workers = []\n",
    "    labels = []\n",
    "    for dir_worker in os.listdir(directory): # Worker\n",
    "\n",
    "        path_worker = directory+\"\\\\\"+dir_worker+\"\\\\\"\n",
    "        if os.path.isdir(path_worker):\n",
    "                for dir_type in os.listdir(path_worker): # true/false\n",
    "                    path_type = path_worker + \"\\\\\" + dir_type + \"\\\\\"\n",
    "                    if dir_type == \"false_claims\":\n",
    "                        label = 1\n",
    "                    else:\n",
    "                        label = 0\n",
    "                    for file in os.listdir(path_type):\n",
    "                        file_path =Path(path_type + \"\\\\\" + file)\n",
    "                        \n",
    "                        files.append(file)\n",
    "                        paths.append(file_path)\n",
    "                        workers.append(dir_worker)\n",
    "                        labels.append(label)\n",
    "\n",
    "    df = pd.DataFrame(list(zip(files, paths, workers, labels)),\n",
    "                columns =['file_name', 'path', 'task','label'])\n",
    "    df.to_csv( root_path+type+\"_df.csv\",index = False)\n",
    "    return df\n",
    "\n",
    "train_df = create_df(train_dir,\"train\")\n",
    "test_df = create_df(test_dir,\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split each subject to support and query sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paths = df.path\n",
    "names = df.file_name\n",
    "workers =df.WorkerId\n",
    "claims = df.IsTrueClaim\n",
    "\n",
    "\n",
    "def sample_files (pathlist):\n",
    "\n",
    "    # list all files in dir\n",
    "    files = [pathlist+file for file in os.listdir(pathlist)]\n",
    "    # select 2 of the files randomly \n",
    "    random_files = np.random.choice(files, 2)\n",
    "\n",
    "    return random_files\n",
    "\n",
    "def split(directory):\n",
    "\n",
    "    for dir in os.listdir(directory):\n",
    "        f = os.path.join(directory, dir)\n",
    "        if os.path.isdir(f):\n",
    "            if not path.isdir(f+\"\\\\support\\\\\"):\n",
    "                os.mkdir(f+\"\\\\support\\\\\")\n",
    "            if not path.isdir(f+\"\\\\query\\\\\"):\n",
    "                os.mkdir(f+\"\\\\query\\\\\")\n",
    "            if not path.isdir(f+\"\\\\support\\\\true_claims\"):\n",
    "                os.mkdir(f+\"\\\\support\\\\true_claims\")\n",
    "            if not path.isdir(f+\"\\\\support\\\\false_claims\"):\n",
    "                os.mkdir(f+\"\\\\support\\\\false_claims\")\n",
    "            if not path.isdir(f+\"\\\\query\\\\true_claims\"):\n",
    "                os.mkdir(f+\"\\\\query\\\\true_claims\")\n",
    "            if not path.isdir(f+\"\\\\query\\\\false_claims\"):\n",
    "                os.mkdir(f+\"\\\\query\\\\false_claims\")\n",
    "\n",
    "            # filelist = glob.glob(os.path.join(f+\"\\\\support\\\\\", \"*/*.wav\"))\n",
    "            # for f in filelist:\n",
    "            #     os.remove(f)\n",
    "        \n",
    "            true_path = f+\"\\\\true_claims\\\\\"\n",
    "            false_path = f+\"\\\\false_claims\\\\\"\n",
    "\n",
    "            # Sample 2 files from true and from false and move it to the support dir.\n",
    "            support_files = sample_files(false_path).tolist()\n",
    "            support_files+=(sample_files(true_path).tolist())\n",
    "\n",
    "            for file in support_files:\n",
    "                dest_path = f + \"\\\\support\\\\\"+file.split(\"\\\\\")[-2]+\"\\\\\"+file.split(\"\\\\\")[-1]\n",
    "                print(file,dest_path)\n",
    "                if os.path.isfile(file):\n",
    "                    \"\"\n",
    "                    os.replace(file, dest_path)\n",
    "\n",
    "            # Move rest of the files to query folder and delete the original folders\n",
    "            for file in os.listdir(true_path):\n",
    "                dest_path = f + \"\\\\query\\\\true_claims\"+\"\\\\\"+file\n",
    "                if os.path.isfile(true_path+\"\\\\\"+file):\n",
    "                    os.replace(true_path+\"\\\\\"+file, dest_path)\n",
    "\n",
    "            for file in os.listdir(false_path):\n",
    "                dest_path = f +  \"\\\\query\\\\false_claims\"+\"\\\\\"+file\n",
    "                if os.path.isfile(false_path+\"\\\\\"+file):\n",
    "                    os.replace(false_path+\"\\\\\"+file, dest_path)\n",
    "            \n",
    "            os.rmdir(true_path)\n",
    "            os.rmdir(false_path)\n",
    "\n",
    "\n",
    "\n",
    "split(train_dir)\n",
    "split(test_dir)\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create df of train and test division\n",
    "\n",
    "[file, path , worker(task), label, set (support/ query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "division_path = root_path + \"train_test_division\\\\\"\n",
    "train_dir = division_path + \"train\\\\\"\n",
    "test_dir = division_path + \"test\\\\\"\n",
    "\n",
    "def create_df(directory,type=\"\"):\n",
    "    files = []\n",
    "    paths = []\n",
    "    workers = []\n",
    "    labels = []\n",
    "    sets = []\n",
    "    for dir_worker in os.listdir(directory): # Worker\n",
    "\n",
    "        path_worker = directory+\"\\\\\"+dir_worker+\"\\\\\"\n",
    "        if os.path.isdir(path_worker):\n",
    "            for dir_set in os.listdir(path_worker): #set \n",
    "                path_set = path_worker + \"\\\\\" + dir_set + \"\\\\\"\n",
    "                for dir_type in os.listdir(path_set): # true/false\n",
    "                    path_type = path_set + \"\\\\\" + dir_type + \"\\\\\"\n",
    "                    if dir_type == \"false_claims\":\n",
    "                        label = 1\n",
    "                    else:\n",
    "                        label = 0\n",
    "                    for file in os.listdir(path_type):\n",
    "                        file_path =Path(path_type + \"\\\\\" + file)\n",
    "                        \n",
    "                        files.append(file)\n",
    "                        paths.append(file_path)\n",
    "                        workers.append(dir_worker)\n",
    "                        labels.append(label)\n",
    "                        sets.append(dir_set)\n",
    "\n",
    "    df = pd.DataFrame(list(zip(files, paths, workers, labels, sets)),\n",
    "                columns =['file_name', 'path', 'task','label','set'])\n",
    "    df.to_csv( root_path+type+\"_df.csv\",index = False)\n",
    "\n",
    "create_df(train_dir,\"train\")\n",
    "create_df(test_dir,\"test\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cae273f1c2fae6daf1f9aea786daf13c24810fe42e58d619ebf469e9980cf618"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
